{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = 2 * np.random.rand(100, 1)\n",
    "y = 4 + 3 * X + np.random.rand(100, 1)\n",
    "\n",
    "X_b = np.c_[np.ones((100, 1)), X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4.2 경사 하강법\n",
    "'''\n",
    "아주 다른 방법으로 선형 회귀 모델을 훈련시켜보자.\n",
    "이 방법은 특성이 매우 많고 훈련 샘플이 너무 많아 메모리에 모두 담을 수 없을 때 적합하다.\n",
    "\n",
    "Gradient Descent\n",
    "\n",
    "lr(Learning Rate) 설정이 중요함 (스탭의 크기)\n",
    "\n",
    "경사 하강법에는 지역 최솟값과 평지라는 두가지 문제점이 있다.\n",
    "다행히 선형 회귀를 위한 MSE 비용 함수는 볼록함수이다.\n",
    "\n",
    "경사하강법을 사용할 때는 반드시 모든 특성이 같은 스케일을 갖도록 만들어야 한다. (사이킷런의 StandartScaler)\n",
    "그렇지 않으면, 수렴하는 데 훨씬 오래 걸린다.\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.50733233]\n",
      " [2.99336016]]\n"
     ]
    }
   ],
   "source": [
    "# 4.2.1 배치 경사 하강법\n",
    "'''\n",
    "각 모델 파라미터 θ에 대해 비용함수의 그레이디언트를 계산해야 한다.\n",
    "== 편도함수\n",
    "\n",
    "매 경사 하강법 스탭에서 전체 훈련 세트 X에 대해 계산합니다. -> 배치 경사 하강법 (매우 큰 훈련세트에서 아주 느림)\n",
    "\n",
    "경사 하강법은 특성수에 민감하지 않음\n",
    "    -> 수십만개의 특성에서 선형 회귀를 훈련시키려면 정규방정식이나 SVD 분해보다 경사 하강법을 사용하는 것이 훨씬 빠름\n",
    "\n",
    "θ = θ - lr * 편도함수값(기울기)\n",
    "'''\n",
    "eta = 0.1 # lr\n",
    "n_iterations = 1000\n",
    "m = 100\n",
    "\n",
    "theta = np.random.randn(2,1) # 무작위 초기화 θ시작값\n",
    "\n",
    "for iteration in range(n_iterations):\n",
    "    gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y) # 편도함수 계산\n",
    "    theta = theta - eta * gradients # 이동\n",
    "\n",
    "print(theta)\n",
    "\n",
    "'''\n",
    "정규방정식으로 찾은 것과 정확히 같다?\n",
    "선형 회귀라 가능한 듯\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2.2 확률적 경사 하강법\n",
    "'''\n",
    "배치 경사 하강법의 문제점 -> 매 스탭에서 전체 훈련세트를 사용해 느리다는 것\n",
    "\n",
    "확률적 경사 하강법 : 매 스탭에서 한 개의 샘플을 무작위로 선택, 그레이디언트를 계산\n",
    "특징 : 불안정함. 지역 최솟값을 건너뛰도록 도와주므로 전역 최솟값을 찾을 가능성이 높음. 근데 다다르지는 못함.\n",
    " -> 이 딜레마를 해결하는 한 가지 방법 : 학습률을 점진적으로 감소시키는 것 (담금질 기법 알고리즘과 유사)\n",
    "\n",
    "간단한 학습 스케줄을 사용한 확률적 경사 하강법을 살펴보자\n",
    "'''\n",
    "import numpy as np\n",
    "\n",
    "n_epochs = 50\n",
    "t0, t1 = 5, 50 # 학습률을 결정하는 학습스케줄 파라미터\n",
    "\n",
    "def learning_schedule(t):\n",
    "    return t0 / (t + t1)\n",
    "\n",
    "theta = np.random.randn(2,1) # 무작위 시작점\n",
    "\n",
    "for epoch in range(n_epochs): # 전체 에포크\n",
    "    for i in range(m): # 샘플 수만큼 반복\n",
    "        random_index = np.random.randint(m) # 랜덤 샘플 선택\n",
    "        xi = X_b[random_index: random_index+1]\n",
    "        yi = y[random_index: random_index+1]\n",
    "        gradients = 2 * xi.T.dot(xi.dot(theta) - yi) # 선택된 샘플로 계산\n",
    "        eta = learning_schedule(epoch * m + i)\n",
    "        theta = theta - eta * gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.49883203],\n",
       "       [2.99244544]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDRegressor(eta0=0.1, penalty=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "sgd_reg = SGDRegressor(max_iter=1000, tol=1e-3, penalty=None, eta0=0.1) # 반복 수 / 오차 허용 / 규제 여부 / 학습률\n",
    "sgd_reg.fit(X, y.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([4.4966062]), array([2.98708747]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_reg.intercept_, sgd_reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2.3 미니배치 경사하강법\n",
    "'''\n",
    "각 스탭에서 전체 훈련세트나 하나의 샘플을 기반으로 그레이디언트를 계산하는 것이 아니라\n",
    "미니배치라 부르는 임의의 작은 샘플 세트에 대해 그레이디언트를 계산한다\n",
    "장점 : 행렬 연산에 최적화된 하드웨어(GPU)를 사용해서 얻는 성능 향상\n",
    "       어느 정도 크게 하면 SGD보다 덜 불규칙하게 움직임 (지역 최솟값에서는 더 못빠져 나올 수도)\n",
    "'''\n",
    "pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
